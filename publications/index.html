<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Toy Leksut | publications</title>
  <meta name="description" content="Toy Story
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">hello</a>

        <!-- Publications -->
        <a class="page-link" href="/publications/">publications</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="/blog/">blog</a> -->

        <!-- Pages -->
<!--         
          
        
          
        
          
        
          
            <a class="page-link" href="/projects/">projects</a>
          
        
          
            <a class="page-link" href="/publications/">publications</a>
          
        
          
            <a class="page-link" href="/resources/">resources</a>
          
        
          
        
          
        
          
         -->

        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <h5 class="post-description">The science of knowing what you donâ€™t and the art of looking like you do -- <a href="https://twitter.com/academicssay" target="_blank">@AcademicsSay</a></h5>
  </header>

  <article class="post-content publications clearfix">
    <h3 class="year">2020</h3>
<div class="pub_img" width="100%">
	<img width="15%" src="/assets/img/2019_viva_0.png" />
	<img width="30%" src="/assets/img/2019_viva_6.png" />
	<img width="20%" src="/assets/img/2019_viva_7.png" />
	<img width="25%" src="/assets/img/2019_viva_plot_wb.png" />
</div>
<ol class="bibliography"><li>

<div id="imavis2020/viva">
  
    <span class="title">Learning Visual Variation for Object Recognition</span>
    <span class="author">
      
        
          
            
              <em>Leksut, J. T.</em>,
            
          
        
      
        
          
            
              
                Zhao, J.,
              
            
          
        
      
        
          
            
              
                and Itti, L.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Image and Vision Computing</em>
    
    
      2020
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [DOI: <a href="https://doi.org/10.1016/j.imavis.2020.103912" target="_blank">10.1016/j.imavis.2020.103912</a>]
  
  
  
  
  
  
  
  
  
    [<a href="/projects/viva" target="_blank">Project</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We propose visual variation learning to improve object recognition with convolutional neural networks (CNN). While a typical CNN regards visual variations as nuisances and marginalizes them from the data, we speculate that some variations are informative. We study the impact of visual variation as an auxiliary task, during training only, on classification and similarity embedding problems. To train the network, we introduce the iLab-20M dataset, a large-scale controlled parametric dataset of toy vehicle objects under systematic annotated variations of viewpoint, lighting, focal setting, and background. After training, we strip out the network components related to visual variations, and test classification accuracy on images with no visual variation labels. Our experiments on 1.75 million images from iLab-20M show significant improvement in object recognition accuracy, i.e., AlexNet: 84.49% to 91.15%; ResNet: 86.14% to 90.70%; and DenseNet: 85.56% to 91.55%. Our key contribution is that, at the cost of visual variation annotation during training only, CNN enhanced with visual variation learning learns better object representations, reducing classification error rate of Alexnet by 42%, ResNet by 32%, and DenseNet by 41%, without significant sacrificing of training time and model complexity.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2019</h3>
<div class="pub_img" width="100%">
	<img width="50%" src="/assets/img/2019_pami_face_1.png" />
	<img width="18%" src="/assets/img/2019_PAMI_face_2.png" />
	<img width="20%" src="/assets/img/2019_pami_face_3.png" />
</div>
<ol class="bibliography"><li>

<div id="pami2019/face">
  
    <span class="title">Learning Pose-Aware Models for Pose-Invariant Face Recognition in the Wild</span>
    <span class="author">
      
        
          
            
              
                Masi, I.,
              
            
          
        
      
        
          
            
              
                Chang, F.,
              
            
          
        
      
        
          
            
              
                Choi, J.,
              
            
          
        
      
        
          
            
              
                Harel, S.,
              
            
          
        
      
        
          
            
              
                Kim, J.,
              
            
          
        
      
        
          
            
              
                Kim, K.,
              
            
          
        
      
        
          
            
              <em>Leksut, J.</em>,
            
          
        
      
        
          
            
              
                Rawls, S.,
              
            
          
        
      
        
          
            
              
                Wu, Y.,
              
            
          
        
      
        
          
            
              
                Hassner, T.,
              
            
          
        
      
        
          
            
              
                AbdAlmageed, W.,
              
            
          
        
      
        
          
            
              
                Medioni, G.,
              
            
          
        
      
        
          
            
              
                Morency, L.,
              
            
          
        
      
        
          
            
              
                Natarajan, P.,
              
            
          
        
      
        
          
            
              
                and Nevatia, R.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>
    
    
      2019
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [DOI: <a href="https://doi.org/10.1109/TPAMI.2018.2792452" target="_blank">10.1109/TPAMI.2018.2792452</a>]
  
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We propose a method designed to push the frontiers of unconstrained face recognition in the wild with an emphasis on extreme out-of-plane pose variations. Existing methods either expect a single model to learn pose invariance by training on massive amounts of data or else normalize images by aligning faces to a single frontal pose. Contrary to these, our method is designed to explicitly tackle pose variations. Our proposed Pose-Aware Models (PAM) process a face image using several pose-specific, deep convolutional neural networks (CNN). 3D rendering is used to synthesize multiple face poses from input images to both train these models and to provide additional robustness to pose variations at test time. Our paper presents an extensive analysis of the IARPA Janus Benchmark A (IJB-A), evaluating the effects that landmark detection accuracy, CNN layer selection, and pose model selection all have on the performance of the recognition pipeline. It further provides comparative evaluations on IJB-A and the PIPA dataset. These tests show that our approach outperforms existing methods, even surprisingly matching the accuracy of methods that were specifically fine-tuned to the target dataset.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2016</h3>
<div class="pub_img" width="100%">
	<img width="25%" src="/assets/img/2016_eccv_face_1.png" />
	<img width="25%" src="/assets/img/2016_eccv_face_2.png" />
	<img width="20%" src="/assets/img/2016_eccv_face_3.png" />
	<img width="20%" src="/assets/img/2016_eccv_face_4.png" />
</div>
<ol class="bibliography"><li>

<div id="eccv2016/face">
  
    <span class="title">Do we really need to collect millions of faces for effective face recognition?</span>
    <span class="author">
      
        
          
            
              
                Masi, I.,
              
            
          
        
      
        
          
            
              
                Tran, A. T.,
              
            
          
        
      
        
          
            
              
                Hassner, T.,
              
            
          
        
      
        
          
            
              <em>Leksut, J.</em>,
            
          
        
      
        
          
            
              
                and Medioni, G.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In European Conference on Computer Vision (ECCV)</em>
    
    
      2016
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [DOI: <a href="https://doi.org/10.1007/978-3-319-46454-1_35" target="_blank">10.1007/978-3-319-46454-1_35</a>]
  
  
    [<a href="http://arxiv.org/abs/1603.07057" target="_blank">arXiv</a>]
  
  
  
  
  
    [<a href="http://www.eccv2016.org/files/posters/S-4B-09.pdf" target="_blank">Poster</a>]
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Face recognition capabilities have recently made extraordinary leaps. Though this progress is at least partially due to ballooning training set sizes â€“ huge numbers of face images downloaded and labeled for identity â€“ it is not clear if the formidable task of collecting so many images is truly necessary. We propose a far more accessible means of increasing training data sizes for face recognition systems: Domain specific data augmentation. We describe novel methods of enriching an existing dataset with important facial appearance variations by manipulating the faces it contains. This synthesis is also used when matching query images represented by standard convolutional neural networks. The effect of training and testing with synthesized images is tested on the LFW and IJB-A (verification and identification) benchmarks and Janus CS2. The performances obtained by our approach match state of the art results reported by systems trained on millions of downloaded images.</p>
  </span>
  
</div>
</li></ol>

<div class="pub_img" width="100%">
	<img width="50%" src="/assets/img/2016_wacv_face_1.png" />
	<img width="45%" src="/assets/img/2016_wacv_face_2.png" />
</div>
<ol class="bibliography"><li>

<div id="wacv2016/face">
  
    <span class="title">Face recognition using deep multi-pose representations</span>
    <span class="author">
      
        
          
            
              
                AbdAlmageed, W.,
              
            
          
        
      
        
          
            
              
                Wu, Y.,
              
            
          
        
      
        
          
            
              
                Rawls, S.,
              
            
          
        
      
        
          
            
              
                Harel, S.,
              
            
          
        
      
        
          
            
              
                Hassner, T.,
              
            
          
        
      
        
          
            
              
                Masi, I.,
              
            
          
        
      
        
          
            
              
                Choi, J.,
              
            
          
        
      
        
          
            
              <em>Leksut, J.</em>,
            
          
        
      
        
          
            
              
                Kim, J.,
              
            
          
        
      
        
          
            
              
                Natarajan, P.,
              
            
          
        
      
        
          
            
              
                Nevatia, R.,
              
            
          
        
      
        
          
            
              
                and Medioni, G.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE Winter Conference on Applications of Computer Vision (WACV)</em>
    
    
      2016
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [DOI: <a href="https://doi.org/10.1109/WACV.2016.7477555" target="_blank">10.1109/WACV.2016.7477555</a>]
  
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We introduce our method and system for face recognition using multiple pose-aware deep learning models. In our representation, a face image is processed by several pose-specific deep convolutional neural network (CNN) models to generate multiple pose-specific features. 3D rendering is used to generate multiple face poses from the input image. Sensitivity of the recognition system to pose variations is reduced since we use an ensemble of pose-specific CNN features. The paper presents extensive experimental results on the effect of landmark detection, CNN layer selection and pose model selection on the performance of the recognition pipeline. Our novel representation achieves better results than the state-of-the-art on IARPAâ€™s CS2 and NISTâ€™s IJB-A in both verification and identification (i.e. search) tasks.</p>
  </span>
  
</div>
</li></ol>

<div class="pub_img" width="100%">
	<img width="30%" src="/assets/img/2016_icpr_face_1.png" />
	<img width="20%" src="/assets/img/2016_icpr_face_3.png" />
	<img width="20%" src="/assets/img/2016_icpr_face_2.png" />
</div>
<ol class="bibliography"><li>

<div id="icpr2016/face">
  
    <span class="title">Expression invariant 3D face modeling from an RGB-D video</span>
    <span class="author">
      
        
          
            
              
                Kim, D.,
              
            
          
        
      
        
          
            
              
                Choi, J.,
              
            
          
        
      
        
          
            
              <em>Leksut, J. T.</em>,
            
          
        
      
        
          
            
              
                and Medioni, G.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In International Conference on Pattern Recognition (ICPR)</em>
    
    
      2016
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [DOI: <a href="https://doi.org/10.1109/ICPR.2016.7899989" target="_blank">10.1109/ICPR.2016.7899989</a>]
  
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We aim to reconstruct an accurate neutral 3D face model from an RGB-D video in the presence of extreme expression changes. Since each depth frame, taken by a low-cost sensor, is noisy, point clouds from multiple frames can be registered and aggregated to build an accurate 3D model. However, direct aggregation of multiple data produces erroneous results in natural interaction (e.g., talking and showing expressions). We propose to analyze facial expression from an RGB frame and neutralize the corresponding 3D point cloud if needed. We first estimate the personâ€™s expression by fitting blendshape coefficients using 2D facial landmarks for each frame and calculate an expression deformity (expression score). With the estimated expression score, we determine whether an input face is neutral or non-neutral. If the face is non-neutral, we proceed to neutralize the expression of the 3D point cloud in that frame. To neutralize the 3D point cloud of a face, we deform our generic 3D face model by applying the estimated blendshape coefficients, find displacement vectors from the deformed generic face to a neutral generic face, and apply the displacement vectors to the input 3D point cloud. After preprocessing frames in a video, we rank frames based on the expression scores and register the ranked frames into a single 3D model. Our system produces a neutral 3D face model in the presence of extreme expression changes even when neutral faces do not exist in the video.</p>
  </span>
  
</div>
</li></ol>

<div class="pub_img" width="100%">
	<img width="20%" src="/assets/img/2016_icip_face_2.png" />
	<img width="55%" src="/assets/img/2016_icip_face_3.png" />
</div>
<ol class="bibliography"><li>

<div id="icip2016/face">
  
    <span class="title">Accurate 3D face modeling and recognition from RGB-D stream in the presence of large pose changes</span>
    <span class="author">
      
        
          
            
              
                Kim, D.,
              
            
          
        
      
        
          
            
              
                Choi, J.,
              
            
          
        
      
        
          
            
              <em>Leksut, J. T.</em>,
            
          
        
      
        
          
            
              
                and Medioni, G.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Image Processing (ICIP)</em>
    
    
      2016
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [DOI: <a href="https://doi.org/10.1109/ICIP.2016.7532912" target="_blank">10.1109/ICIP.2016.7532912</a>]
  
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We propose a 3D face modeling and recognition system using an RGB-D stream in the presence of large pose changes. In the previous work, all facial data points are registered with a reference to improve the accuracy of 3D face model from a low-resolution depth sequence. This registration often fails when applied to non-frontal faces. It causes inaccurate 3D face models and poor performance of matching. We address this problem by pre-aligning each input face (â€˜frontalizationâ€™) before the registration, which avoids registration failures. For each frame, our method estimates the 3D face pose, assesses the quality of data, segments the facial region, frontalizes it, and performs an accurate registration with the previous 3D model. The 3D-3D recognition system using accurate 3D models from our method outperforms other face recognition systems and shows 100% rank 1 recognition accuracy on a dataset with 30 subjects.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2013</h3>
<div class="pub_img" width="100%">
	<img width="70%" src="/assets/img/2013_jicce_face_1.png" />
</div>
<ol class="bibliography"><li>

<div id="jicce2013/face">
  
    <span class="title">3D Facial Landmark Tracking and Facial Expression Recognition</span>
    <span class="author">
      
        
          
            
              
                Medioni, G.,
              
            
          
        
      
        
          
            
              
                Choi, J.,
              
            
          
        
      
        
          
            
              
                Labeau, M.,
              
            
          
        
      
        
          
            
              <em>Leksut, J. T.</em>,
            
          
        
      
        
          
            
              
                and Meng, L.
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Journal of Information and Communication Convergence Engineering</em>
    
    
      2013
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
    [DOI: <a href="https://doi.org/10.6109/jicce.2013.11.3.207" target="_blank">10.6109/jicce.2013.11.3.207</a>]
  
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>In this paper, we address the challenging computer vision problem of obtaining a reliable facial expression analysis from a naturally interacting person. We propose a system that combines a 3D generic face model, 3D head tracking, and 2D tracker to track facial landmarks and recognize expressions. First, we extract facial landmarks from a neutral frontal face, and then we deform a 3D generic face to fit the input face. Next, we use our real-time 3D head tracking module to track a personâ€™s head in 3D and predict facial landmark positions in 2D using the projection from the updated 3D face model. Finally, we use tracked 2D landmarks to update the 3D landmarks. This integrated tracking loop enables efficient tracking of the non-rigid parts of a face in the presence of large 3D head motion. We conducted experiments for facial expression recognition using both framebased and sequence-based approaches. Our method provides a 75.9% recognition rate in 8 subjects with 7 key expressions. Our approach provides a considerable step forward toward new applications including human-computer interactions, behavioral science, robotics, and game applications.</p>
  </span>
  
</div>
</li></ol>

  </article>

  

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2020 Toy Leksut.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>





<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
